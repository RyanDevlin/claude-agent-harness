# Claude Agent Harness

A lightweight harness for running multiple Claude coding agents in parallel. You provide a project spec in markdown, and the harness takes care of the rest — planning tasks, setting up the environment, and coordinating agents so they don't step on each other. Each agent runs in a Docker container, and all coordination happens through git.

Inspired by Anthropic's engineering blog posts on [building effective harnesses for long-running agents](https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents) and [building a C compiler with agents](https://www.anthropic.com/engineering/building-c-compiler).

## How It Works

The harness runs in three phases:

**Phase 1 — Planning.** The first agent to boot reads your `PROJECT_SPEC.md` and generates a `tasks.json` (task breakdown) and an `init.sh` (environment setup script). A git-based lock ensures only one agent plans — the rest wait.

**Phase 2 — Environment Setup.** Every agent runs `init.sh` to install project-specific dependencies (Go, Python, Rust, etc.) on top of the base Ubuntu image.

**Phase 3 — Task Loop.** Each agent repeatedly:
1. Pulls the latest from git
2. Cleans up any stale locks from crashed agents
3. Finds the next pending task
4. Claims it by committing a lock file to `current_tasks/`
5. Runs the Claude CLI to work on the task
6. Commits the work, releases the lock, and pushes

If a task fails, it's automatically requeued for retry (up to `MAX_TASK_RETRIES` attempts). If two agents race for the same task, git's push semantics handle it — the second push fails, that agent backs off and picks a different task. If an agent crashes mid-task, other agents detect the dead lock (via Docker DNS or timeout) and reclaim the task.

**Phase 4 — Validation.** After all tasks reach a terminal state, one agent runs a validation check against `PROJECT_SPEC.md`. If the project meets the spec, agents exit. If gaps are found, the validator creates remediation tasks and agents loop back to Phase 3.

## Project Structure

```
claude-agent-harness/
├── Dockerfile                 # Ubuntu 24.04 + build tools + Claude CLI
├── docker-compose.yml         # Run and scale agents
├── config.env                 # Your configuration (repo URL, model, etc.)
├── .env                       # Docker Compose volume paths (generated by setup.sh)
├── setup.sh                   # One-time setup: generates deploy key and configures harness
├── AGENT_PROMPT.md            # Instructions given to coding agents
├── PLANNER_PROMPT.md          # Instructions given to the planning agent
├── VALIDATOR_PROMPT.md        # Instructions given to the validation agent
├── PROJECT_SPEC.md.example    # Example project specification
└── scripts/
    ├── agent-loop.sh          # Main entrypoint (orchestrates all phases)
    ├── plan-tasks.sh          # Runs the planner agent with locking
    ├── claim-task.sh          # Claims a task via git lock file
    ├── release-task.sh        # Releases a task (with retry logic)
    ├── validate-project.sh    # Runs the validation agent with locking
    └── sync-repo.sh           # Git clone/pull/push helper
```

## Quick Start

### Prerequisites

- Docker and Docker Compose
- An [Anthropic API key](https://console.anthropic.com/)
- A GitHub repo that agents can clone and push to

### 1. Clone this repo

```bash
git clone <this-repo-url> claude-agent-harness
cd claude-agent-harness
```

### 2. Add a project spec to your target repo

Create a `PROJECT_SPEC.md` file in the root of the repo you want agents to work on. This is the only input agents need — they'll read it and figure out the rest. See [Writing a Project Spec](#writing-a-project-spec) below.

### 3. Configure and set up

Edit `config.env` with your repo URL:

```bash
REPO_URL=git@github.com:your-org/your-repo.git
REPO_BRANCH=main
CLAUDE_MODEL=claude-sonnet-4-20250514
```

Then run the setup script:

```bash
./setup.sh
```

This will:
1. Generate an SSH deploy key (`~/.ssh/harness_deploy_key`)
2. Walk you through adding it to GitHub (with write access)
3. Test the connection
4. Configure `.env` with the key path

### 4. Export your API key

```bash
export ANTHROPIC_API_KEY=sk-ant-...
```

### 5. Build and run

```bash
# Build the container image
docker compose build

# Run a single agent
docker compose up

# Or run multiple agents in parallel
docker compose up --scale agent=4
```

Agents will automatically:
1. Clone your repo
2. Generate a task plan from your `PROJECT_SPEC.md`
3. Install any project-specific dependencies
4. Start working through tasks, committing and pushing as they go

### 6. Monitor progress

Watch agent logs in your terminal, or check the repo:

```bash
# See what tasks exist and their status
git pull && cat tasks.json | jq '.[] | {id, status}'

# See which tasks are currently being worked on
ls current_tasks/

# See agent commit history
git log --oneline
```

## Configuration Reference

All options go in `config.env`:

| Variable | Default | Description |
|----------|---------|-------------|
| `REPO_URL` | *(required)* | Git remote URL (SSH or HTTPS) |
| `REPO_BRANCH` | `main` | Branch to work on |
| `CLAUDE_MODEL` | `claude-sonnet-4-20250514` | Claude model for planning and coding |
| `MAX_ITERATIONS` | `0` | Max task loop iterations per agent (0 = unlimited) |
| `LOCK_STALE_MINUTES` | `30` | Minutes before a lock from a crashed agent is reclaimed |
| `MAX_TASK_RETRIES` | `2` | Max retries for failed tasks (0 = no retries) |
| `ENABLE_VALIDATION` | `true` | Run validation phase after all tasks complete |
| `MAX_VALIDATION_ROUNDS` | `2` | Max validation rounds (prevents infinite remediation) |
| `AGENT_PROMPT_FILE` | `/harness/AGENT_PROMPT.md` | Absolute path to the coding agent prompt (inside container) |
| `PLANNER_PROMPT_FILE` | `/harness/PLANNER_PROMPT.md` | Absolute path to the planner prompt (inside container) |
| `VALIDATOR_PROMPT_FILE` | `/harness/VALIDATOR_PROMPT.md` | Absolute path to the validator prompt (inside container) |

Additionally, `ANTHROPIC_API_KEY` must be set in your shell environment (not in `config.env`).

## Writing a Project Spec

The `PROJECT_SPEC.md` in your target repo is the single source of truth for agents. It should include:

- **What to build** — clear description of the project
- **Technical constraints** — language, frameworks, storage, ports
- **Feature requirements** — what the system should do
- **Testing expectations** — how to verify things work
- **Project structure** — suggested directory layout (optional but helpful)

See `PROJECT_SPEC.md.example` for a complete example (a Go URL shortener API).

The planner agent reads this spec and produces:
- **`tasks.json`** — ordered list of small, independent tasks
- **`init.sh`** — script to install project-specific tools (Go, Python, etc.)
- **`DECISIONS.md`** — documents any choices made when the spec was ambiguous

## Git Authentication

The recommended approach is to use the `setup.sh` script, which generates a dedicated SSH deploy key and configures everything automatically.

If you prefer to manage keys manually:

1. Generate a key: `ssh-keygen -t ed25519 -f ~/.ssh/harness_deploy_key -N ""`
2. Add it as a [deploy key](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/managing-deploy-keys) on your repo with **write access**
3. Set the path in `.env`:
   ```
   SSH_KEY_PATH=/absolute/path/to/.ssh/harness_deploy_key
   ```

For **HTTPS URLs**, you can use a personal access token in the URL:

```bash
REPO_URL=https://<token>@github.com/your-org/your-repo.git
```

## Task Retries

When a task fails (Claude CLI exits non-zero), it's automatically requeued as `pending` with an incremented `retry_count`. This handles transient failures like API credit exhaustion, rate limits, or network issues. On retry, the agent's prompt includes context about the previous failure so Claude can adapt its approach.

A task is marked permanently `failed` only after exhausting `MAX_TASK_RETRIES` (default: 2) retries, meaning each task gets up to 3 total attempts.

Set `MAX_TASK_RETRIES=0` in `config.env` to disable retries.

## Validation

After all tasks reach a terminal state (done or permanently failed), the harness runs a validation phase. One agent claims a validation lock and runs the Claude CLI with a validation prompt that:

1. Reads `PROJECT_SPEC.md` and reviews the codebase
2. Runs tests and checks for build errors
3. Identifies any gaps between the spec and the implementation

If the project passes, the validator creates a `VALIDATION_PASSED` file and all agents exit. If gaps are found, the validator appends new remediation tasks (prefixed with `fix-`) to `tasks.json`, and agents loop back to work on them.

`MAX_VALIDATION_ROUNDS` (default: 2) prevents infinite remediation loops. Set `ENABLE_VALIDATION=false` to skip validation entirely.

## Crash Recovery

The harness handles agent crashes automatically. When an agent dies mid-task, its lock file remains in git. Other agents detect dead locks in two ways:

1. **DNS liveness** — Each agent checks if the lock holder's container is still running via Docker Compose's internal DNS (`getent hosts <agent-id>`). If the container is gone, the lock is reclaimed immediately.
2. **Timeout fallback** — If the DNS check passes but the lock is older than `LOCK_STALE_MINUTES` (default: 30), it's treated as stale and reclaimed.

Reclaimed tasks are reset to `pending` so another agent picks them up.

## Troubleshooting

**Agents can't push (permission denied)**
- Run `./setup.sh` to generate and configure a deploy key
- Verify the deploy key has write access on GitHub
- Check that `.env` has the correct absolute path to the key

**Agents can't find `PROJECT_SPEC.md`**
- The file must be committed to the target repo's root, on the branch specified in `REPO_BRANCH`

**Planning phase takes a long time**
- Planning uses the Claude CLI which can take several minutes for large specs, especially with Opus
- The planner's output streams to your terminal in real-time so you can monitor progress
- Check the planner log inside the container: `/tmp/planner_*.log`

**Agents stuck on old locks after a crash**
- This resolves automatically — agents detect dead containers via DNS and stale locks via timeout
- To force cleanup: `docker compose down` removes all containers, then re-run
- Increase `LOCK_STALE_MINUTES` in `config.env` if tasks legitimately take longer than 30 minutes

**Agent IDs start at a high number (e.g., 10 instead of 1)**
- Docker Compose tracks container numbers incrementally across runs
- Run `docker compose down` before `docker compose up` to reset numbering

**Two agents claimed the same task**
- This shouldn't happen — the git-based locking prevents it. If you see it, check that agents can push to the remote (lock files must be committed and pushed to work)

**Agent exited immediately**
- If all tasks in `tasks.json` are `done` or `in_progress`, agents exit cleanly. Check `tasks.json` for task statuses.
